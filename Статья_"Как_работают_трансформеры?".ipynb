{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roo9yjX2BGev"
      },
      "source": [
        "# Как работают трансформеры?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruATlzpTBNK_"
      },
      "source": [
        "## Подготовка данных:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    !nvidia-smi\n",
        "    !pip install cupy-cuda12x\n",
        "    import cupy as np\n",
        "    if not np.cuda.runtime.getDeviceCount():\n",
        "        raise ImportError\n",
        "    print(\"Using GPU\")\n",
        "except:\n",
        "    import numpy as np\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "import random\n",
        "from typing import Union, Optional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Mhl77_pciN",
        "outputId": "dd14dc18-2a7c-47b5-f88b-4867173fc474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Collecting cupy-cuda12x\n",
            "  Downloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy<2.6,>=1.22 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (2.0.2)\n",
            "Collecting fastrlock>=0.5 (from cupy-cuda12x)\n",
            "  Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Downloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl (112.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastrlock, cupy-cuda12x\n",
            "Successfully installed cupy-cuda12x-13.6.0 fastrlock-0.8.3\n",
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZVcNnk75onF"
      },
      "outputs": [],
      "source": [
        "# !pip install cupy-cuda12x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT72kt0cBTUl"
      },
      "source": [
        "## **Создание функционального трансформера:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9JcOGlQCENF"
      },
      "outputs": [],
      "source": [
        "#@title Глобальные параметры:\n",
        "batch = 64\n",
        "std = .05\n",
        "heads = 4\n",
        "d_model = 32\n",
        "part_size = int(d_model / heads)  # Не менять\n",
        "ff_multiply = 4\n",
        "vocab_size = 12                   # Включая токены Start и Finish\n",
        "max_tokens = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Al1_CtBdBf"
      },
      "outputs": [],
      "source": [
        "#@title Инициализация обучаемых параметров:\n",
        "\n",
        "# Параметры внимания (для энкодера и декодера):\n",
        "size1 = (2, heads, part_size, part_size)\n",
        "size2 = (2, d_model, d_model)\n",
        "MWQ = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "MWK = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "MWV = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "MWO = np.random.normal(size=size2, scale=std).astype(np.float32)\n",
        "\n",
        "# Параметры перекрестного внимания:\n",
        "size1 = (heads, part_size, part_size)\n",
        "size2 = (d_model, d_model)\n",
        "CWQ = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "CWK = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "CWV = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "CWO = np.random.normal(size=size2, scale=std).astype(np.float32)\n",
        "\n",
        "# Параметры прямого прохода:\n",
        "size1 = (2, d_model, d_model * ff_multiply)\n",
        "size2 = (2, d_model * ff_multiply, d_model)\n",
        "size3 = (2, d_model * ff_multiply)\n",
        "size4 = (2, d_model)\n",
        "FW1 = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "FW2 = np.random.normal(size=size2, scale=std).astype(np.float32)\n",
        "FB1 = np.zeros(size3).astype(np.float32)\n",
        "FB2 = np.zeros(size4).astype(np.float32)\n",
        "\n",
        "# Параметры выходного слоя:\n",
        "size1 = (d_model, vocab_size)\n",
        "size2 = (vocab_size)\n",
        "WO = np.random.normal(size=size1, scale=std).astype(np.float32)\n",
        "BO = np.zeros(size2).astype(np.float32)\n",
        "\n",
        "# Обучаемые параметры (для оптимизатора):\n",
        "parameters = [\n",
        "    MWQ[0], MWK[0], MWV[0], MWO[0],\n",
        "    FW1[0], FB1[0], FW2[0], FB2[0],\n",
        "    MWQ[1], MWK[1], MWV[1], MWO[1],\n",
        "    CWQ, CWK, CWV, CWO,\n",
        "    FW1[1], FB1[1], FW2[1], FB2[1],\n",
        "    WO, BO,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwwQaqOHB2vo"
      },
      "outputs": [],
      "source": [
        "#@title Работа с вокабуляром:\n",
        "def build_fixed_embeddings(vocab_size: int, random_state: int = 13) -> np.ndarray:\n",
        "    \"\"\"Ортогональные эмбеддинги\"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    mat = rng.standard_normal(size=(d_model, d_model))\n",
        "    Q, _ = np.linalg.qr(mat)\n",
        "    emb = Q[:vocab_size]\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "\n",
        "vocab = build_fixed_embeddings(vocab_size)\n",
        "Start = 10\n",
        "Finish = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEmXLgu3B2in"
      },
      "outputs": [],
      "source": [
        "#@title Вспомогательные функции:\n",
        "def softmax(X: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Softmax по последней оси\"\"\"\n",
        "    shifted = X - np.max(X, axis=-1, keepdims=True)\n",
        "    exp = np.exp(shifted)\n",
        "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def leakyrelu(X: np.ndarray, rate: float = .1) -> np.ndarray:\n",
        "    \"\"\"Leaky ReLU\"\"\"\n",
        "    return np.where(X > 0, X, rate * X).astype(np.float32)\n",
        "\n",
        "\n",
        "def add_start_token(data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Присоединение стартового токена в начало последовательности\"\"\"\n",
        "    size = (data.shape[0], 1)\n",
        "    start = np.tile(Start, size).astype(np.int32)  # Стартовый токен (B, 1)\n",
        "    return np.concatenate((start, data), axis=1)   # Стартовый набор (B, 1+T)\n",
        "\n",
        "\n",
        "def add_finish_token(data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Присоединение финишного токена в конец последовательности\"\"\"\n",
        "    size = (data.shape[0], 1)\n",
        "    finish = np.tile(Finish, size).astype(np.int32)  # Финишный токен (B, 1)\n",
        "    return np.concatenate((data, finish), axis=1)    # Финишный набор (B, T+1)\n",
        "\n",
        "def predict(data: np.ndarray, dtype=np.int32) -> np.ndarray:\n",
        "    \"\"\"Выдает предсказание для данных\"\"\"\n",
        "    predictions = []\n",
        "    for batch in data:\n",
        "        predictions.append(call(batch))\n",
        "    return np.array(predictions, dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk8m7stxCYBF"
      },
      "outputs": [],
      "source": [
        "#@title Основные слои:\n",
        "def positional_encoding(X: np.ndarray) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"Позиционное кодирование токенов (наложение частот)\"\"\"\n",
        "    B, T, D = X.shape\n",
        "\n",
        "    # Номера позиций токенов:\n",
        "    positions = np.arange(T, dtype=np.float32)[:, None]  # (T, 1)\n",
        "\n",
        "    # Матрица частот:\n",
        "    i = np.arange(D // 2, dtype=np.float32)         # Индексы эмбеддингов\n",
        "    angles = positions * 10000 ** (-2 * i / D)      # (1, D)\n",
        "\n",
        "    pe = np.zeros((T, D), dtype=np.float32)         # (T, D)\n",
        "    pe[:, 0::2] = np.sin(angles)                    # Для чётных индексов\n",
        "    pe[:, 1::2] = np.cos(angles)                    # Для нечётных\n",
        "\n",
        "    pe = np.tile(pe, (B, 1, 1)).astype(np.float32)  # (B, T, D)\n",
        "\n",
        "    return X + pe                                   # Наложение частот\n",
        "\n",
        "def normalize(X: np.ndarray, eps: float = 1e-8):\n",
        "    \"\"\"Простая нормализация без масштабирования и обучаемых параметров\"\"\"\n",
        "    rms = np.sqrt(np.mean(X**2, axis=-1, keepdims=True) + eps)  # (B T 1)\n",
        "    cache = {\n",
        "        \"X\": X,\n",
        "        \"rms\": rms\n",
        "    }\n",
        "    return X / rms, cache\n",
        "\n",
        "def MHA(X: np.ndarray, is_decoder: bool = False, index: int = 0, neg_inf: float = -1e9) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"Многоголовое внимание\"\"\"\n",
        "    T = X.shape[1]\n",
        "\n",
        "    # Разделение входной матрицы для мультивнимания:\n",
        "    X = X.reshape(X.shape[0], T, heads, part_size)     # (B T H P)\n",
        "\n",
        "    # Матрицы проекций:\n",
        "    Q = np.einsum('bthd, hdp -> bhtp', X, MWQ[index])  # (B T H P)(H P P) (B H T P)\n",
        "    K = np.einsum('bthd, hdp -> bhtp', X, MWK[index])  # (B T H P)(H P P) (B H T P)\n",
        "    V = np.einsum('bthd, hdp -> bhtp', X, MWV[index])  # (B T H P)(H P P) (B H T P)\n",
        "\n",
        "    # Матрица внимания:\n",
        "    A = np.einsum('bhtp, bhfp -> bhtf', Q, K)          # (B H T P)(B H T P) (B H T T)\n",
        "    A = A / np.sqrt(part_size, dtype=np.float32)       # Масштабирование\n",
        "\n",
        "    # Маскирование внимания:\n",
        "    if is_decoder:\n",
        "        size = (X.shape[0], heads, T, T)\n",
        "        mask = np.triu(\n",
        "            np.full(size, neg_inf),\n",
        "            k=1\n",
        "        ).astype(np.float32)\n",
        "        A = A + mask\n",
        "\n",
        "    # Матрица взвешенны значений:\n",
        "    A_softmax = softmax(A)\n",
        "    Z = np.einsum('bhtf, bhfp -> bhtp', A_softmax, V)  # (B H T T)(B H T P) (B H T P)\n",
        "\n",
        "    # Объединение голов:\n",
        "    shape = (X.shape[0], T, d_model)\n",
        "    Zo = Z.transpose(0, 2, 1, 3)                       # (B H T P) (B T H P)\n",
        "    Zo = Zo.reshape(shape)                             # (B T H P) (B T D)\n",
        "    Zo = np.einsum('btd, df -> btf', Zo, MWO[index])   # (B T D)(D D) (B T D)\n",
        "\n",
        "    cache = {\n",
        "        \"X\": X, \"Q\": Q, \"K\": K, \"V\": V,\n",
        "        \"A\": A, \"attn\": A_softmax, \"Z\": Z,\n",
        "        \"index\": index, \"mask\": mask if is_decoder else None,\n",
        "    }\n",
        "    return Zo, cache\n",
        "\n",
        "\n",
        "def MHCA(X: np.ndarray, context: np.ndarray) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"Многоголовое перекрестное внимание\"\"\"\n",
        "\n",
        "    # Разделение входной матрицы для мультивнимания:\n",
        "    new_shape_X = (X.shape[0], X.shape[1], heads, part_size)\n",
        "    new_shape_Z = (context.shape[0], context.shape[1], heads, part_size)\n",
        "    X = X.reshape(new_shape_X)                         # (B Tx H P)\n",
        "    context = context.reshape(new_shape_Z)             # (B Tz H P)\n",
        "\n",
        "    # Матрицы проекций:\n",
        "    Q = np.einsum('bthd, hdp -> bhtp', X, CWQ)         # (B Tx H P)(H P P) (B H Tx P)\n",
        "    K = np.einsum('bthd, hdp -> bhtp', context, CWK)   # (B T H P)(H P P) (B H Tz P)\n",
        "    V = np.einsum('bthd, hdp -> bhtp', context, CWV)   # (B T H P)(H P P) (B H Tz P)\n",
        "\n",
        "    # Матрица внимания:\n",
        "    A = np.einsum('bhxp, bhtp -> bhxt', Q, K)          # (B H Tx P)(B H Tz P) (B H Tx Tz)\n",
        "    A = A / np.sqrt(part_size, dtype=np.float32)\n",
        "    A_softmax = softmax(A)\n",
        "\n",
        "    # Матрица взвешенных значений:\n",
        "    Z = np.einsum('bhxt, bhtp -> bhxp', A_softmax, V)  # (B H Tx Tz)(B H Tz P) (B H Tx P)\n",
        "\n",
        "    # Объединение голов:\n",
        "    Zo = Z.transpose(0, 2, 1, 3)                       # (B H Tx P) (B Tx H P)\n",
        "    shape = (X.shape[0], X.shape[1], d_model)\n",
        "    Zo = Zo.reshape(shape)                             # (B Tx H P) (B Tx D)\n",
        "    Zo = np.einsum('btd, df -> btf', Zo, CWO)          # (B Tx D)(D D) (B Tx D)\n",
        "\n",
        "    cache = {\n",
        "        \"X\": X, \"context\": context, \"Q\": Q, \"K\": K, \"V\": V,\n",
        "        \"A\": A, \"attn\": A_softmax, \"Z\": Z,\n",
        "    }\n",
        "    return Zo, cache\n",
        "\n",
        "def FF(X: np.ndarray, index: int = 0) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"Полносвязный блок с расширением\"\"\"\n",
        "    X1 = np.einsum('BTD, DP -> BTP', X, FW1[index])   # (B T D)(D D) (B T D*ff)\n",
        "    X2 = X1 + FB1[index]                              # (D*ff,) (B T D*ff)\n",
        "    X3 = leakyrelu(X2)\n",
        "    X4 = np.einsum('BTD, DP -> BTP', X3, FW2[index])  # (B T D)(D*ff D) (B T D)\n",
        "    X5 = X4 + FB2[index]                              # (D,) (B T D)\n",
        "    return X5, {\n",
        "        \"X\": X, \"W1\": X1, \"B1\": X2,\n",
        "        \"act\": X3, \"W2\": X4, \"index\": index\n",
        "    }\n",
        "\n",
        "def output(X: np.ndarray) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"Конечный линейный слой проекции словаря\"\"\"\n",
        "    Y = np.einsum('BTD, DV -> BTV', X, WO)  # (B T D)((D V) (B T V)\n",
        "    Y = Y + BO                              # (B T V)\n",
        "    return Y, {\"X\": X}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK5CNl7RC3AA"
      },
      "outputs": [],
      "source": [
        "#@title Обратные функции слоев:\n",
        "def backward_normalize(dY: np.ndarray, cache: dict) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Форма dY: (B, T, D)\n",
        "    Возврат:\n",
        "        dX: (B, T, D)\n",
        "    \"\"\"\n",
        "    X = cache[\"X\"]\n",
        "    rms = cache[\"rms\"]\n",
        "    D = X.shape[-1]\n",
        "    return (dY / rms - X * np.sum(dY * X, axis=-1, keepdims=True) / (D * rms**3))\n",
        "\n",
        "def backward_MHA(dZ: np.ndarray, cache: dict) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Форма dZo: (B, Tx, D)\n",
        "    Возврат:\n",
        "        dX: (B, T, D)\n",
        "        grads: словарь градиентов\n",
        "    \"\"\"\n",
        "    X = cache[\"X\"]              # (B T H P)\n",
        "    Q = cache[\"Q\"]              # (B H T P)\n",
        "    K = cache[\"K\"]              # (B H T P)\n",
        "    V = cache[\"V\"]              # (B H T P)\n",
        "    A_softmax = cache[\"attn\"]   # (B H T T)\n",
        "    Z = cache[\"Z\"]              # (B H T P)\n",
        "    index = cache[\"index\"]\n",
        "\n",
        "    B, T, _, _ = X.shape\n",
        "\n",
        "    # Производная общей матрицы WO:\n",
        "    dMWO = np.einsum(\n",
        "        'btd, btf -> df',\n",
        "        Z.transpose(0, 2, 1, 3).reshape(B, T, d_model),\n",
        "        dZ\n",
        "    )                                                         # (B T D1)(B T D2) (D1 D2)\n",
        "    dZ = np.einsum(\n",
        "        'btd, fd -> btf',\n",
        "        dZ,\n",
        "        MWO[index]\n",
        "    ).reshape(B, T, heads, part_size).transpose(0, 2, 1, 3)   # (B T D)(D1 D2) (B T D1) (B H T P)\n",
        "\n",
        "    # Производная для проекции значений V:\n",
        "    dA = np.einsum('bhtp, bhfp -> bhtf', dZ, V)               # (B H T P)(B H T P) (B H T T)\n",
        "    dV = np.einsum('bhit, bhtp -> bhip', A_softmax, dZ)       # (B H T1 T2)(B H T P) (B H T1 P)\n",
        "\n",
        "    # Обнуление градиентов (если есть маска):\n",
        "    if (mask := cache[\"mask\"]) is not None:\n",
        "        dA = np.where(mask == 0, dA, 0.0).astype(np.float32)\n",
        "\n",
        "    # Производная софтмакса:\n",
        "    tmp = np.sum(dA * A_softmax, axis=-1, keepdims=True)\n",
        "    dA = A_softmax * (dA - tmp)\n",
        "    dA = dA / np.sqrt(part_size, dtype=np.float32)\n",
        "\n",
        "    # Производная внимания:\n",
        "    dQ = np.einsum('bhtf, bhtp -> bhfp', dA, K)              # (B H T1 T2)(B H T1 P) (B H T2 P)\n",
        "    dK = np.einsum('bhtf, bhfp -> bhtp', dA, Q)              # (B H T1 T2)(B H T2 P) (B H T1 P)\n",
        "\n",
        "    # Производная по проекции входных данных:\n",
        "    dMWQ = np.einsum('bthp, bhtf -> hfp', X, dQ)             # (B T H P)(B H T P) (H P P)\n",
        "    dMWK = np.einsum('bthp, bhtf -> hfp', X, dK)             # (B T H P)(B H T P) (H P P)\n",
        "    dMWV = np.einsum('bthp, bhtf -> hfp', X, dV)             # (B T H P)(B H T P) (H P P)\n",
        "\n",
        "    # Производная по входу:\n",
        "    dX = (\n",
        "        np.einsum('bhtp, hfp -> bthf', dQ, MWQ[index]) +     # (B T H P)(H P1 P2) (B T H P1)\n",
        "        np.einsum('bhtp, hfp -> bthf', dK, MWK[index]) +     # (B T H P)(H P1 P2) (B T H P1)\n",
        "        np.einsum('bhtp, hfp -> bthf', dV, MWV[index])       # (B T H P)(H P1 P2) (B T H P1)\n",
        "    ).reshape(B, T, d_model)\n",
        "\n",
        "    return dX, {\n",
        "        \"MWQ\": dMWQ,\n",
        "        \"MWK\": dMWK,\n",
        "        \"MWV\": dMWV,\n",
        "        \"MWO\": dMWO\n",
        "    }\n",
        "\n",
        "\n",
        "def backward_MHCA(dZ: np.ndarray, cache: dict) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Форма dZo: (B, Tx, D)\n",
        "    Возврат:\n",
        "        dX: (B, Tx, D)\n",
        "        dContext: (B, Tz, D)\n",
        "        grads: словарь градиентов\n",
        "    \"\"\"\n",
        "\n",
        "    X = cache[\"X\"]                 # (B Tx H P)\n",
        "    context = cache[\"context\"]     # (B Tz H P)\n",
        "    Q = cache[\"Q\"]                 # (B H Tx P)\n",
        "    K = cache[\"K\"]                 # (B H Tz P)\n",
        "    V = cache[\"V\"]                 # (B H Tz P)\n",
        "    A_softmax = cache[\"attn\"]      # (B H Tx Tz)\n",
        "    Z = cache[\"Z\"]                 # (B H Tx P)\n",
        "\n",
        "    B, Tx, H, P = X.shape\n",
        "    Tz = context.shape[1]\n",
        "\n",
        "    # Производная общей матрицы WO:\n",
        "    dCWO = np.einsum(\n",
        "        'btd, btf -> df',\n",
        "        Z.transpose(0, 2, 1, 3).reshape(B, Tx, d_model),\n",
        "        dZ\n",
        "    )                                                     # (B T D1)(B T D2) (D1 D2)\n",
        "    dZ = np.einsum(\n",
        "        'btd, fd -> btf',\n",
        "        dZ,\n",
        "        CWO\n",
        "    ).reshape(B, Tx, H, P).transpose(0, 2, 1, 3)          # (B Tx D)(D1 D2) (B Tx D1) (B H Tx P)\n",
        "\n",
        "    # Производная для проекции значений V:\n",
        "    dA = np.einsum('bhtp, bhfp -> bhtf', dZ, V)           # (B H Tx P)(B H Tz P) (B H Tx Tz)\n",
        "    dV = np.einsum('bhtf, bhtp -> bhfp', A_softmax, dZ)   # (B H Tx Tz)(B H Tx P) (B H Tz P)\n",
        "\n",
        "    # Производная софтмакса:\n",
        "    tmp = np.sum(dA * A_softmax, axis=-1, keepdims=True)\n",
        "    dA = A_softmax * (dA - tmp)\n",
        "    dA = dA / np.sqrt(part_size, dtype=np.float32)\n",
        "\n",
        "    # Производная внимания:\n",
        "    dQ = np.einsum('bhtf, bhfp -> bhtp', dA, K)           # (B H Tx Tz)(B H Tz P) (B H Tx P)\n",
        "    dK = np.einsum('bhtf, bhtp -> bhfp', dA, Q)           # (B H Tx Tz)(B H Tx P) (B H Tz P)\n",
        "\n",
        "    # Производная по проекциям входных данных:\n",
        "    dCWQ = np.einsum('bthp, bhtf -> hpf', X, dQ)          # (B Tx H P)(B H Tx P) (H P P)\n",
        "    dCWK = np.einsum('bthp, bhtf -> hpf', context, dK)    # (B Tx H P)(B H Tz P) (H P P)\n",
        "    dCWV = np.einsum('bthp, bhtf -> hpf', context, dV)    # (B Tx H P)(B H Tz P) (H P P)\n",
        "\n",
        "    # Производная по входу (X и Z):\n",
        "    dX = np.einsum(\n",
        "        'bhtp, hfp -> bthf', dQ, CWQ\n",
        "    ).reshape(B, Tx, d_model)                             # (B Tx H P)(H P1 P2) (B Tx H P1)\n",
        "    dZ = (\n",
        "        np.einsum('bhtp, hfp -> bthf', dK, CWK) +         # (B Tz H P)(H P1 P2) (B Tz H P1)\n",
        "        np.einsum('bhtp, hfp -> bthf', dV, CWV)           # (B Tz H P)(H P1 P2) (B Tz H P1)\n",
        "    ).reshape(B, Tz, d_model)\n",
        "\n",
        "    grads = {\n",
        "        \"CWQ\": dCWQ,\n",
        "        \"CWK\": dCWK,\n",
        "        \"CWV\": dCWV,\n",
        "        \"CWO\": dCWO\n",
        "    }\n",
        "\n",
        "    return dX, dZ, grads\n",
        "\n",
        "\n",
        "def backward_FF(dX5: np.ndarray, cache: dict) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Форма dX5: (B, T, D)\n",
        "    Возврат:\n",
        "        dX: (B, T, D)\n",
        "        grads: словарь градиентов\n",
        "    \"\"\"\n",
        "    X  = cache[\"X\"]         # (B T D)\n",
        "    X1 = cache[\"W1\"]        # (B T D*ff)\n",
        "    X2 = cache[\"B1\"]        # (D*ff,)\n",
        "    X3 = cache[\"act\"]       # (B T D*ff)\n",
        "    index = cache[\"index\"]  # (i)\n",
        "\n",
        "    # Градиент для Байес 2:\n",
        "    dFB2 = np.sum(dX5, axis=(0, 1))                            # (B T D) (D,)\n",
        "\n",
        "    # Производная линейного слоя 2:\n",
        "    dFW2 = np.einsum('btd, btp -> dp', X3, dX5)                # (B T D*ff)(B T D) (D*ff D)\n",
        "    dX3 = np.einsum('btp, dp -> btd', dX5, FW2[index])         # (B T D)(D*ff D) (B T D*ff)\n",
        "\n",
        "    # Производная нелинейного слоя:\n",
        "    dX2 = dX3 * np.where(X2 > 0, 1.0, 0.1).astype(np.float32)  # (B T D*ff)\n",
        "\n",
        "    # Градиент для Байес 1:\n",
        "    dFB1 = np.sum(dX2, axis=(0, 1))                            # (D*ff,)\n",
        "\n",
        "    # Производная линейного слоя 1:\n",
        "    dFW1 = np.einsum('btd, btp -> dp', X, dX2)                 # (B T D)(D D*ff) (D, D*ff)\n",
        "    dX = np.einsum('btp, dp -> btd', dX2, FW1[index])          # (B T D*ff)(D D*ff) (B T D)\n",
        "\n",
        "    grads = {\n",
        "        \"FW1\": dFW1,\n",
        "        \"FB1\": dFB1,\n",
        "        \"FW2\": dFW2,\n",
        "        \"FB2\": dFB2\n",
        "    }\n",
        "\n",
        "    return dX, grads\n",
        "\n",
        "\n",
        "def backward_output(dY: np.ndarray, cache: dict) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Форма dY: (B, T, V)\n",
        "    Возврат:\n",
        "        dX: (B, T, D)\n",
        "        grads: словарь градиентов\n",
        "    \"\"\"\n",
        "    X = cache[\"X\"]\n",
        "\n",
        "    # Производная сдвига:\n",
        "    dBO = np.sum(dY, axis=(0, 1))             # (V,)\n",
        "\n",
        "    # Производная по весам:\n",
        "    dWO = np.einsum('btd, btv -> dv', X, dY)  # (B T D)(B T V) (D V)\n",
        "\n",
        "    # Производная по входу:\n",
        "    dX = np.einsum('btv, dv -> btd', dY, WO)  # (B T V)(D V) (B T D)\n",
        "\n",
        "    grads = {\n",
        "        \"WO\": dWO,\n",
        "        \"BO\": dBO\n",
        "    }\n",
        "\n",
        "    return dX, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lsfeoXpjdMn"
      },
      "outputs": [],
      "source": [
        "#@title Функция ошибки:\n",
        "def SparseCrossEntropy(prediction: np.ndarray, target: np.ndarray, eps: float = 1e-8) -> tuple[int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    prediction: [B, T, V] — логиты\n",
        "    target: [B, T] — индексы правильных токенов\n",
        "    \"\"\"\n",
        "    probs = softmax(prediction)\n",
        "    B, T, V = probs.shape\n",
        "\n",
        "    correct_probs = probs[np.arange(B)[:, None], np.arange(T)[None, :], target]\n",
        "\n",
        "    loss_value = -np.sum(np.log(correct_probs + eps), dtype=np.float32)\n",
        "    loss_value = loss_value / (B * T)\n",
        "    loss_value = np.array(loss_value, dtype=np.float32)  # ensure scalar-array of correct dtype\n",
        "\n",
        "    grad_prediction = probs.copy().astype(np.float32)\n",
        "    grad_prediction[np.arange(B)[:, None], np.arange(T)[None, :], target] -= 1.0\n",
        "    grad_prediction /= (B * T)\n",
        "\n",
        "    return loss_value, grad_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGBnBpCoC2EF"
      },
      "outputs": [],
      "source": [
        "#@title Прямой проход:\n",
        "def call(X: np.ndarray, Y: np.ndarray = None) -> np.ndarray:\n",
        "    \"\"\"Проход по блокам\"\"\"\n",
        "    # Энкодер:\n",
        "    training = True if Y is not None else False\n",
        "    X = np.take(vocab, X, axis=0)                   # Получение эмбеддингов (B, T, D)\n",
        "    Xr = positional_encoding(X)                      # Позиционный энкодинг\n",
        "\n",
        "    X, cache_Norm1 = normalize(Xr)\n",
        "    X, cache_MHAe = MHA(X, index=0)\n",
        "    Xr = Xr + X\n",
        "    X, cache_Norm2 = normalize(Xr)\n",
        "    X, cache_FFe = FF(X, index=0)\n",
        "    Z = Xr + X\n",
        "\n",
        "    # Декодер:\n",
        "    if training:                                    # Режим обучения\n",
        "        X = add_start_token(Y)                      # Присоединение стартового набора (B, 1+T)\n",
        "        X = np.take(\n",
        "            vocab,\n",
        "            X,\n",
        "            axis=0\n",
        "        ).astype(np.float32)                         # Получение эмбеддингов (B, 1+T, D)\n",
        "        Xr = positional_encoding(X)                  # Позиционное кодирование\n",
        "\n",
        "        X, cache_Norm3 = normalize(Xr)\n",
        "        X, cache_MHAd = MHA(X, index=1, is_decoder=True)\n",
        "        Xr = Xr + X\n",
        "        X, cache_Norm4 = normalize(Xr)\n",
        "        X, cache_MHCA = MHCA(X, Z)\n",
        "        Xr = Xr + X\n",
        "        X, cache_Norm5 = normalize(Xr)\n",
        "        X, cache_FFd = FF(X, index=1)\n",
        "        Xr = Xr + X\n",
        "        X, cache_Norm6 = normalize(Xr)\n",
        "        X, cache_out = output(X)\n",
        "        cache = {\n",
        "            \"Norm1\": cache_Norm1, \"MHA-E\": cache_MHAe,\n",
        "            \"Norm2\": cache_Norm2, \"FF-E\": cache_FFe,\n",
        "            \"Norm3\": cache_Norm3, \"MHA-D\": cache_MHAd,\n",
        "            \"Norm4\": cache_Norm4, \"MHCA\": cache_MHCA,\n",
        "            \"Norm5\": cache_Norm5, \"FF-D\": cache_FFd,\n",
        "            \"Norm6\": cache_Norm6, \"Out\": cache_out\n",
        "        }\n",
        "        return X, cache\n",
        "    else:                                          # Режим генерации\n",
        "        seq = np.tile(\n",
        "            Start, (Z.shape[0], 1)\n",
        "        ).astype(np.int32)                         # Стартовый набор (B, 1)\n",
        "        for _ in range(max_tokens):                # Цикл токенов\n",
        "            X = np.take(vocab, seq, axis=0)        # Получение эмбеддингов (B, 1, D)\n",
        "            Xr = positional_encoding(X)            # Позиционное кодирование\n",
        "\n",
        "            X, _ = normalize(Xr)\n",
        "            X, _ = MHA(X, index=1, is_decoder=True)\n",
        "            Xr = Xr + X\n",
        "            X, _ = normalize(Xr)\n",
        "            X, _ = MHCA(X, Z)\n",
        "            Xr = Xr + X\n",
        "            X, _ = normalize(Xr)\n",
        "            X, _ = FF(X, index=1)\n",
        "            X = Xr + X\n",
        "            X, _ = output(X)\n",
        "\n",
        "            X = softmax(X)                         # Вероятности\n",
        "\n",
        "            indexes = np.argmax(\n",
        "                X[:, -1, :], axis=-1\n",
        "                )[..., None]                       # Получение индексов новых токенов\n",
        "            seq = np.concatenate(\n",
        "                (seq, indexes)\n",
        "                , axis=1\n",
        "            )                                      # Добавление токенов\n",
        "        return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PhzaXnjDNPd"
      },
      "outputs": [],
      "source": [
        "#@title Обратный проход:\n",
        "def backward(loss: np.ndarray, cache) -> tuple[dict, ...]:\n",
        "    \"\"\"\n",
        "    Обратный проход.\n",
        "    Возвращает наборы градиентов для каждого слоя.\n",
        "    \"\"\"\n",
        "    # Декодер:\n",
        "    dX, grads1 = backward_output(loss, cache['Out'])\n",
        "    dXr = backward_normalize(dX, cache['Norm6'])\n",
        "\n",
        "    dX, grads2 = backward_FF(dXr, cache['FF-D'])\n",
        "    dX = backward_normalize(dX, cache['Norm5'])\n",
        "    dXr = dXr + dX\n",
        "\n",
        "    dX, dZ, grads3 = backward_MHCA(dXr, cache['MHCA'])\n",
        "    dX = backward_normalize(dX, cache['Norm4'])\n",
        "    dXr = dXr + dX\n",
        "\n",
        "    dX, grads4 = backward_MHA(dXr, cache['MHA-D'])\n",
        "    # dX = backward_normalize(dX, cache['Norm3'])\n",
        "    # dXr = dXr + dX\n",
        "\n",
        "    # Энкодер:\n",
        "    dX, grads5 = backward_FF(dZ, cache['FF-E'])\n",
        "    dX = backward_normalize(dX, cache['Norm2'])\n",
        "    dXr = dX + dZ\n",
        "\n",
        "    dX, grads6 = backward_MHA(dXr, cache['MHA-E'])\n",
        "    # dX = backward_normalize(dX, cache['Norm1'])\n",
        "    # dXr = dXr + dX\n",
        "\n",
        "    return grads6, grads5, grads4, grads3, grads2, grads1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcwvkJs9Inig",
        "outputId": "a651c3de-c633-427c-b0cb-b3d6df408551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHA (64, 16, 32)\n",
            "MHCA (64, 16, 32)\n",
            "FF (64, 16, 32)\n",
            "Out (64, 16, 12) \n",
            "\n",
            "dOut (64, 16, 32)\n",
            "dFF (64, 16, 32)\n",
            "dMHCA (64, 16, 32) (64, 16, 32)\n",
            "dMHA (64, 16, 32) \n",
            "\n",
            "Call generate (64, 17)\n",
            "Call training (64, 17, 12)\n",
            "(64, 17)\n",
            "Loss  2.1832407\n",
            "Backward grads:\n",
            "dict_keys(['MWQ', 'MWK', 'MWV', 'MWO'])\n",
            "dict_keys(['FW1', 'FB1', 'FW2', 'FB2'])\n",
            "dict_keys(['MWQ', 'MWK', 'MWV', 'MWO'])\n",
            "dict_keys(['CWQ', 'CWK', 'CWV', 'CWO'])\n",
            "dict_keys(['FW1', 'FB1', 'FW2', 'FB2'])\n",
            "dict_keys(['WO', 'BO'])\n"
          ]
        }
      ],
      "source": [
        "#@title Проверочный код:\n",
        "size = (batch, max_tokens, d_model)\n",
        "data = np.random.random(size).astype(np.float32)\n",
        "\n",
        "# Проверка основных слоев:\n",
        "X, cache_MHA = MHA(data, index=0)\n",
        "assert X.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"MHA\", X.shape)\n",
        "\n",
        "X, cache_MHCA = MHCA(data, X)\n",
        "assert X.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"MHCA\", X.shape)\n",
        "\n",
        "X, cache_FF = FF(X, index=0)\n",
        "assert X.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"FF\", X.shape)\n",
        "\n",
        "X, cache_out = output(X)\n",
        "assert (size[0], size[1], vocab_size) == X.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"Out\", X.shape, '\\n')\n",
        "\n",
        "# Проверка обратных слоев:\n",
        "dX, grads = backward_output(X, cache_out)\n",
        "assert dX.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"dOut\", dX.shape)\n",
        "\n",
        "dX, cache = backward_FF(dX, cache_FF)\n",
        "assert dX.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"dFF\", dX.shape)\n",
        "\n",
        "dX, dZ, cache = backward_MHCA(dX, cache_MHCA)\n",
        "assert dX.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "assert dZ.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"dMHCA\", dX.shape, dZ.shape)\n",
        "\n",
        "dX, grads = backward_MHA(dX, cache_MHA)\n",
        "assert dX.shape == data.shape, \"Формы данных входа и выхода не совпали!\"\n",
        "print(\"dMHA\", dX.shape, '\\n')\n",
        "\n",
        "# Проверка верхнеуровневых функций:\n",
        "size = (batch, max_tokens)\n",
        "data = np.random.randint(0, 9, size=size, dtype=np.int32)\n",
        "\n",
        "# Инференс:\n",
        "out = call(data)\n",
        "assert out.shape == (batch, max_tokens + 1), f\"Выходной размер не соответствует ожидаемому!\"\n",
        "print(\"Call generate\", out.shape)\n",
        "\n",
        "# Режим обучения:\n",
        "out, cache = call(data, data)\n",
        "assert out.shape == (batch, max_tokens + 1, vocab_size), f\"Выходной размер не соответствует ожидаемому!\"\n",
        "print(\"Call training\", out.shape)\n",
        "\n",
        "# Проверка Loss-функции:\n",
        "target = np.argmax(softmax(out), axis=-1)\n",
        "print(target.shape)\n",
        "loss, grad = SparseCrossEntropy(out, target)\n",
        "# assert round(float(loss)) == round(np.log(vocab_size)), f\"Лосс не совпал с ожидаемым!\"\n",
        "print(\"Loss \", loss)\n",
        "\n",
        "# Обратный проход:\n",
        "grads = backward(out, cache)\n",
        "assert len(grads) == 6, f\"Не все слои получили градиенты!\"\n",
        "print(\"Backward grads:\")\n",
        "for g in grads:\n",
        "    print(g.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yt-aD6MBZLG"
      },
      "source": [
        "## **Подготовка к обучению модели:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MySOLZqIZ1H"
      },
      "outputs": [],
      "source": [
        "#@title Генератор данных:\n",
        "class Generator:\n",
        "    def __init__(\n",
        "            self, batch_size: int,\n",
        "            number_tokens: int, number_batch: int,\n",
        "            shuffle: bool = True, test_size: float = .2,\n",
        "            dtype=np.int32\n",
        "    ) -> None:\n",
        "        self.batch_size = batch_size\n",
        "        self.number_tokens = number_tokens\n",
        "        self.number_batch = number_batch\n",
        "        self.shuffle = shuffle\n",
        "        self.test_size = test_size\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def palindrome(self) -> Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"Создание последовательностий с переворотом второй половины\"\"\"\n",
        "        data, target = list(), list()\n",
        "        self.number_tokens //= 2\n",
        "        for _ in range(self.number_batch):\n",
        "            data_batch, target_batch = list(), list()\n",
        "            for num in range(self.batch_size):\n",
        "                half = str(random.randint(10 ** (self.number_tokens - 1), 10 ** self.number_tokens - 1))\n",
        "                reverse = list(half)\n",
        "                reverse.reverse()\n",
        "                data_batch.append([int(i) for i in \"\".join((half, half))])\n",
        "                target_batch.append([int(i) for i in \"\".join((half, \"\".join(reverse)))])\n",
        "            data.append(data_batch)\n",
        "            target.append(target_batch)\n",
        "\n",
        "        data = np.array((data, target), dtype=self.dtype).transpose(1, 0, 2, 3)\n",
        "        return self.train_test_split(data, self.test_size) if self.shuffle else data\n",
        "\n",
        "    def symmetric_sum(self) -> Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"Симметричная сумма T[i] = (S[i] + S[N−1−i]) % 10\"\"\"\n",
        "        data, target = [], []\n",
        "        for _ in range(self.number_batch):\n",
        "            batch_input, batch_target = [], []\n",
        "            for _ in range(self.batch_size):\n",
        "                # Случайная входная последовательность:\n",
        "                S = [random.randint(0, 9) for _ in range(self.number_tokens)]\n",
        "                T = [(S[i] + S[self.number_tokens - 1 - i]) % 10 for i in range(self.number_tokens)]\n",
        "                batch_input.append(S)\n",
        "                batch_target.append(T)\n",
        "            data.append(batch_input)\n",
        "            target.append(batch_target)\n",
        "\n",
        "        data = np.array((data, target), dtype=self.dtype).transpose(1, 0, 2, 3)\n",
        "        return self.train_test_split(data, self.test_size) if self.shuffle else data\n",
        "\n",
        "\n",
        "    def train_test_split(self, data: np.ndarray, test_size: float = .2) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Разбиение данных\"\"\"\n",
        "        idx = np.random.permutation(len(data))  # Случайный порядок индексов\n",
        "        data = data[idx]\n",
        "        index = int(len(data) * (1 - test_size))\n",
        "        return data[:index], data[index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w110Q4z1Bdei"
      },
      "outputs": [],
      "source": [
        "#@title Функция обучения модели:\n",
        "def fit(\n",
        "        data_train: np.ndarray, data_valid: np.ndarray,\n",
        "        epoch: int, shuffle: bool = False, step_per_epoch: int = None\n",
        ") -> None:\n",
        "    \"\"\"\"\"\"\n",
        "    # Проверка количества шагов:\n",
        "    if not step_per_epoch:\n",
        "        step_per_epoch = data_train.shape[0]\n",
        "\n",
        "    # Цикл обучения:\n",
        "    state = dict()\n",
        "    for i in range(1, epoch + 1):\n",
        "        # Метрики:\n",
        "        general_loss_train = 0\n",
        "        general_loss_valid = 0\n",
        "\n",
        "        # Тасование батчей:\n",
        "        if shuffle:\n",
        "            idx = np.random.permutation(len(data_train))  # Случайный порядок индексов\n",
        "            data_train = data_train[idx]\n",
        "\n",
        "        # Обучающая часть:\n",
        "        for step, (batch, target) in enumerate(data_train[:step_per_epoch], start=1):\n",
        "            prediction, cache = call(batch, Y=target)\n",
        "\n",
        "            loss, grad = SparseCrossEntropy(\n",
        "                prediction,\n",
        "                add_finish_token(target)\n",
        "            )\n",
        "\n",
        "            grad_parameters = backward(grad, cache)\n",
        "\n",
        "            state = optimizer(parameters, grad_parameters, step, state)        # Оптимизация\n",
        "\n",
        "            general_loss_train += float(loss)\n",
        "            print('=', end='')\n",
        "\n",
        "        # Валидационная часть:\n",
        "        for batch, target in data_valid:\n",
        "            prediction, cache = call(batch, Y=target)\n",
        "            loss, grad = SparseCrossEntropy(prediction, add_finish_token(target))\n",
        "\n",
        "            general_loss_valid += float(loss)\n",
        "\n",
        "        # Вывод ошибки:\n",
        "        general_loss_train /= step_per_epoch\n",
        "        general_loss_valid /= data_valid.shape[0]\n",
        "\n",
        "        print(f\"\\nЭпоха: {i}\")\n",
        "        print(f\"Ошибка модели на обучении: {general_loss_train}\")\n",
        "        print(f\"Ошибка модели на тесте: {general_loss_valid}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ywxf5zb7laL"
      },
      "outputs": [],
      "source": [
        "# def sheduler(step):\n",
        "#     \"\"\"Вычисляет скорость на нужно шаге\"\"\"\n",
        "#     lr = finish_rate + (start_rate - finish_rate) * (total_steps - step) / total_steps\n",
        "#     return lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7chkT_mm1UmR"
      },
      "outputs": [],
      "source": [
        "def sheduler(step, warmup_steps=128):\n",
        "    \"\"\"Вычисляет скорость на нужно шаге\"\"\"\n",
        "    if step <= warmup_steps:\n",
        "        return start_rate * step / warmup_steps\n",
        "    else:\n",
        "        return (start_rate - finish_rate) * (total_steps - step) / total_steps\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHo0RG2LIX_D"
      },
      "outputs": [],
      "source": [
        "#@title Оптимизатор Адам:\n",
        "def optimizer(parameters: list, gradients: dict, step: int) -> None:\n",
        "    \"\"\"СГС\"\"\"\n",
        "    gradients = [elem for values in gradients for elem in values.values()]\n",
        "    for parameter, gradient in zip(parameters, gradients):\n",
        "        global_g = sheduler(step)\n",
        "        parameter -= global_g * gradient\n",
        "\n",
        "def optimizer(\n",
        "        parameters: list, gradients: list,\n",
        "        step: int, state: dict,\n",
        "        beta1=0.9, beta2=0.999, eps=1e-8\n",
        ") -> dict:\n",
        "    \"\"\"Оптимизатор Адам\"\"\"\n",
        "\n",
        "    if 'm' not in state:\n",
        "        state['m'] = [np.zeros_like(p) for p in parameters]\n",
        "        state['v'] = [np.zeros_like(p) for p in parameters]\n",
        "\n",
        "    gradients = [elem for values in gradients for elem in values.values()]\n",
        "    for i, (p, g) in enumerate(zip(parameters, gradients)):\n",
        "        # Моменты:\n",
        "        state['m'][i] = beta1 * state['m'][i] + (1 - beta1) * g\n",
        "        state['v'][i] = beta2 * state['v'][i] + (1 - beta2) * (g * g)\n",
        "\n",
        "        # Байес:\n",
        "        m_hat = state['m'][i] / (1 - beta1 ** step)\n",
        "        v_hat = state['v'][i] / (1 - beta2 ** step)\n",
        "\n",
        "        # Обновление параметра:\n",
        "        p -= sheduler(step) * m_hat / (np.sqrt(v_hat) + eps)\n",
        "\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YfshsFPQfi"
      },
      "source": [
        "## **Обучение модели:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEFDD0-NC4Yn",
        "outputId": "a4ab0b5d-207c-4529-d940-7f1cab6524fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape:  (171, 2, 64, 16)\n",
            "Valid shape:  (85, 2, 64, 16)\n"
          ]
        }
      ],
      "source": [
        "#@title Создание данных:\n",
        "gen = Generator(\n",
        "    batch_size=batch,\n",
        "    number_tokens=max_tokens,\n",
        "    number_batch=256,\n",
        "    test_size=.33\n",
        ")\n",
        "\n",
        "train, valid = gen.palindrome()   # symmetric_sum, palindrome\n",
        "\n",
        "print(\"Train shape: \", train.shape)\n",
        "print(\"Valid shape: \", valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdWLvK46HTw"
      },
      "outputs": [],
      "source": [
        "#@title Прараметры обучения:\n",
        "epoch = 10\n",
        "steps_per_epoch = 64  # Количество подаваемых батчей за одну эпоху\n",
        "start_rate = 1e-2\n",
        "finish_rate = 1e-5\n",
        "warmup_steps = 128    # Количество \"шагов прогрева\"\n",
        "total_steps = epoch * steps_per_epoch - warmup_steps  # Общее количество шагов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW9ZkwDWIXUI"
      },
      "outputs": [],
      "source": [
        "#@title Обучение модели:\n",
        "fit(\n",
        "    data_train=train, data_valid=valid,\n",
        "    shuffle=True, step_per_epoch=steps_per_epoch,\n",
        "    epoch=epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX-C9F44TrCY"
      },
      "source": [
        "## Тестирование модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GSr23BITaiK",
        "outputId": "c2fdcbd0-cdfe-4590-cca8-a4d3e26cca71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Вывод: \n",
            "[1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[8 7 6 5 4 3 2 1 8 7 6 5 4 3 2 1] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[0 1 0 2 0 3 0 4 0 1 0 2 0 3 0 4] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[1 1 2 3 1 1 4 5 1 1 2 3 1 1 4 5] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[5 5 4 4 3 3 2 2 5 5 4 4 3 3 2 2] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[7 7 7 7 8 8 9 9 7 7 7 7 8 8 9 9] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[1 5 3 7 9 2 4 6 1 5 3 7 9 2 4 6] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n",
            "[3 9 2 6 1 2 5 6 3 9 2 6 1 2 5 6] -> [10  9 11  9  8  8  8  8  8  8  8  8  9  9  3  9  2]\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка модели:\n",
        "print(\"\\nВывод: \")\n",
        "request = np.array([[\n",
        "    [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    [8, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "    [0, 1, 0, 2, 0, 3, 0, 4, 0, 1, 0, 2, 0, 3, 0, 4],\n",
        "    [1, 1, 2, 3, 1, 1, 4, 5, 1, 1, 2, 3, 1, 1, 4, 5],\n",
        "    [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
        "    [5, 5, 4, 4, 3, 3, 2, 2, 5, 5, 4, 4, 3, 3, 2, 2],\n",
        "    [7, 7, 7, 7, 8, 8, 9, 9, 7, 7, 7, 7, 8, 8, 9, 9],\n",
        "    [1, 5, 3, 7, 9, 2, 4, 6, 1, 5, 3, 7, 9, 2, 4, 6],\n",
        "    [3, 9, 2, 6, 1, 2, 5, 6, 3, 9, 2, 6, 1, 2, 5, 6],\n",
        "]])\n",
        "\n",
        "prediction = predict(request)\n",
        "for r, p in zip(request[0], prediction[0]):\n",
        "    print(f\"{r} -> {p}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnb20ALViHU2SZQRFwM4u9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}